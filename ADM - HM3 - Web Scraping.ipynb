{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beautifulsoup to work with the pages\n",
    "from bs4 import BeautifulSoup\n",
    "#used to access the pages without having problem to screaping\n",
    "import requests\n",
    "#work with dataframes and better manipulation of data\n",
    "import pandas as pd\n",
    "#used to create folders and files\n",
    "import os\n",
    "#used to create the .tsv files\n",
    "import csv\n",
    "#a particular string matches a given regular expression \n",
    "import re\n",
    "#use to detect the language\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.1\n",
    "#define an array to load the urls\n",
    "url = []\n",
    "#this for is to load the pages\n",
    "for i in range(1, 301):\n",
    "    #request access to the page number i\n",
    "    page = requests.get(\"https://www.goodreads.com/list/show/1.Best_Books_Ever?page=\"+ str(i))\n",
    "    #read properly the html page to scrape data\n",
    "    soup = BeautifulSoup(page.content, features='lxml')\n",
    "    #find the url of the given book\n",
    "    find_a = soup.find_all('a',class_=\"bookTitle\", itemprop = \"url\")\n",
    "    #in each page we have 100 books, this inside for is to fill the array with the url of each book on the page\n",
    "    for j in range(0,100):\n",
    "        #to add the book url on the array\n",
    "        url.append(\"https://www.goodreads.com\"+ find_a[j]['href'])\n",
    "        \n",
    "#create a txt file where for each row there is a book's url \n",
    "with open(\"url.txt\", 'w') as file:\n",
    "    #write on the url.txt the url of the book\n",
    "    file.write(\"\\n\".join(map(str, url)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "1.2\n",
    "#open the url.txt and read all the lines\n",
    "file = open('url.txt', \"r\")\n",
    "lines=file.readlines()#pass the lines to a array/list\n",
    "file.close()\n",
    "#HERE YOU CHANGE THE DIRECTORY TO SAVE THE HTML IN YOUR OWN COMPUTER, THIS IS MY CHOOSEN PATH FOR MY PC\n",
    "path=\"D:/ADM/Homework 3/database\"\n",
    "#THIS FIRST FOR IS TO LOAD THE PAGES\n",
    "#MY SUGGESTION\n",
    "#- Mateus DOWNLOAD FROM 1 TO 101\n",
    "#- Ousainou DOWNLOAD FROM 101 TO 201\n",
    "#- Camila DOWNLOAD FROM 201 TO 301\n",
    "#TOTAL OF PAGES ADJUSTED TO THE FOR INDEX - FROM 1 TO 301 - TOTAL 300 PAGES\n",
    "for page in range(1,301):\n",
    "    name=\"page\"+str(page)\n",
    "    pathmaster = os.path.join(path, name)\n",
    "    os.makedirs(pathmaster)\n",
    "    #AS EACH PAGE HAS 100 BOOKS, DO NOT CHANGE THIS FOR BECAUSE IT IS RESPONSIBLE TO SEPARETE THE CORRECT NUMBER OF BOOK FOR EACH PAGE\n",
    "    for line in range (1,101):\n",
    "        site=lines[line]\n",
    "        access=requests.get(site, allow_redirects=True)\n",
    "        book_site = \"article_\" + str(line) + \".html\"\n",
    "        open(pathmaster+\"/\"+book_site,'wb').write(access.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#open the url.txt and read all the lines\n",
    "file = open('url.txt', \"r\")\n",
    "lines=file.readlines()#pass the lines to a array/list\n",
    "file.close()\n",
    "#HERE YOU CHANGE THE DIRECTORY TO SAVE THE HTML IN YOUR OWN COMPUTER, THIS IS MY CHOOSEN PATH FOR MY PC\n",
    "for i in range(1,301):\n",
    "    #here is the access of where we are gonna save the tsv files\n",
    "    page_folder_path=\"D:/ADM/Homework 3/database/page\"+str(i)+\"/\"\n",
    "    for j in range(1,101):\n",
    "        #for each article of the page, first we get the less complex informations as mentioned below\n",
    "        article_saved='article_'+str(j)+'.html'\n",
    "        teste=str(page_folder_path+article_saved)\n",
    "        soup = BeautifulSoup(open(teste, 'r', encoding=\"utf8\"), features='lxml')\n",
    "        bookTitle = str(soup.find_all('h1', id = \"bookTitle\")[0].contents[0]).replace('\\n', '').strip()\n",
    "        bookAuthors = str(soup.find_all('a', {\"class\": \"authorName\"})[0].contents[0].contents[0])\n",
    "        ratingValue = soup.find_all('span', itemprop = \"ratingValue\")[0].contents[0].replace('\\n', '')\n",
    "        ratingCount = soup.find_all('a', href=\"#other_reviews\")[0].contents[2].replace('\\n', '').strip().split()[0]\n",
    "        reviewCount = soup.find_all('a', href=\"#other_reviews\")[1].contents[2].replace('\\n', '').strip().split()[0]\n",
    "        NumberofPages = soup.find_all('span', itemprop=\"numberOfPages\")[0].contents[0]\n",
    "        Published = soup.find_all('div', {\"class\": \"row\"})[1].contents[0].replace('\\n', '').strip().split(\" \"*8)[1]\n",
    "        aux=str(soup.find_all('link',rel='canonical')).split()\n",
    "        Url=aux[1].strip('href=').strip('\"')\n",
    "        aux1 = soup.find_all('div', {\"class\": \"infoBoxRowTitle\"})\n",
    "        index = False\n",
    "        for stri in aux1:\n",
    "            if 'Series' in stri:\n",
    "                index = aux1.index(stri)\n",
    "        bookSeries = \"\"\n",
    "        if index:\n",
    "            bookSeries += str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[index].contents[1]).strip().split(\">\")[1][:-3]\n",
    "        #now we treat PLOT, CHARACTERS AND SETTINGS because we need an different approach to clean the information\n",
    "        #PLOT\n",
    "        #first of all, define the full description\n",
    "        if (len(soup.find_all('div', id=\"description\")[0].contents) <= 3):\n",
    "            descrip = soup.find_all('div', id=\"description\")[0].contents[1].contents\n",
    "        else :\n",
    "            descrip = soup.find_all('div', id=\"description\")[0].contents[3].contents\n",
    "            #if the description begins with this kind of description, remove it\n",
    "        if len(soup.find_all('div', id=\"description\")[0].contents) > 3 :\n",
    "            if len(soup.find_all('div', id=\"description\")[0].contents[3].contents) > 2:\n",
    "                if \"<i>\" in str(descrip[0]):\n",
    "                    descrip = descrip[3:]\n",
    "            #remove the <br/> tags and add to the list\n",
    "        Plot = []\n",
    "        for q in range(len(descrip)):\n",
    "            if str(descrip[q]) != '<br/>':\n",
    "                Plot.append(descrip[q])\n",
    "            # remove the <i> and </i> tags\n",
    "        for w in range(len(Plot)):\n",
    "            if '<i>' in str(Plot[w]):\n",
    "                Plot[w] = re.sub(r'<i>', '', str(Plot[w]))\n",
    "                Plot[w] = re.sub(r'</i>', '', str(Plot[w]))\n",
    "\n",
    "            #if the plot begins in bold, remove the <b> tags\n",
    "        if \"<b>\" in str(Plot[0]):\n",
    "            Plot[0] = Plot[0].contents[0]\n",
    "            \n",
    "        for u in range(len(Plot)):\n",
    "            Plot[u] = str(Plot[u])\n",
    "                \n",
    "                \n",
    "        Plot = \" \".join(Plot)\n",
    "\n",
    "        if Plot == [] :\n",
    "                \n",
    "            Plot = \"\"\n",
    "\n",
    "        #CHARACTERS\n",
    "        index = False\n",
    "        #using the same soup created above, we check if there is a setting inside the soup    \n",
    "        for stri in aux1:\n",
    "            if 'Characters' in stri:\n",
    "                index = aux1.index(stri)\n",
    "        characters = []\n",
    "        #if character exists, we will find them up to 5 characters.\n",
    "        if index:\n",
    "            if int(len(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[index].contents)/2) <= 5 :\n",
    "                for e in range(int(len(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[index].contents)/2)):\n",
    "        #using 2i+1 because character is on odd index - name and surname\n",
    "                    characters.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[index].contents[2*e+1]).strip().split(\">\")[1][:-3])\n",
    "            else:\n",
    "                for e in range(5):\n",
    "                    characters.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[index].contents[2*e+1]).strip().split(\">\")[1][:-3])\n",
    "            characters = \", \".join(characters)\n",
    "        else :\n",
    "            characters = \"\"\n",
    "        setting = []\n",
    "        index = 0\n",
    "        for check in aux1:\n",
    "        #using the same soup created above, we check if there is a setting inside the soup    \n",
    "            if 'Setting' in check:\n",
    "                index = aux1.index(check)\n",
    "        if index > 0:\n",
    "            for r in range(2):\n",
    "                if \"</s\" in str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[index].contents[2*r+1]) :\n",
    "                        temp = re.sub(r'</s', '', str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[index].contents[2*r+1]))\n",
    "                        setting.append((str(temp).strip().split(\">\")[1][:-3]).strip())\n",
    "                else:\n",
    "                    setting.append(str(soup.find_all('div', {\"class\": \"infoBoxRowItem\"})[index].contents[2*r+1]).strip().split(\">\")[1][:-3])\n",
    "            setting = \" \".join(setting)\n",
    "        else :\n",
    "            setting = \"\"\n",
    "        #DEFINE IF PLOT IS IN ENGLISH\n",
    "        if detect(Plot) == 'en':     \n",
    "            out_path = page_folder_path + \"article_\" + str(j) + \".tsv\"\n",
    "        with open(out_path, 'wt', encoding=\"utf8\") as out_file:\n",
    "            tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "            tsv_writer.writerow(['bookTitle', 'bookAuthors', 'ratingValue', 'ratingCount', 'reviewCount', 'Plot', 'NumberofPages', 'Published', 'Characters', 'Setting', 'Url'])\n",
    "            tsv_writer.writerow([bookTitle, bookSeries, bookAuthors, ratingValue, ratingCount, reviewCount, Plot, NumberofPages, Published, characters, setting, Url])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
